{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query that returns the rate_type, loan_id, loan balance , \n",
    "# and a column that shows with what percentage the loan's balance \n",
    "# contributes to the total balance among the loans of the same rate type.\n",
    "\n",
    "df = submissions\n",
    "rate_type_total = df.groupby('rate_type')['balance'].sum().to_frame('balance_total').reset_index()\n",
    "df = pd.merge(df, rate_type_total, on='rate_type')\n",
    "df['contribution%'] = df.apply(lambda x: (x['balance'] / x['balance_total']) * 100, axis=1)\n",
    "df = df[['id', 'rate_type', 'loan_id', 'balance', 'contribution%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fraction (percentage divided by 100) of rides each weather-hour combination constitutes among all weather-hour combinations.\n",
    "# Output the weather, hour along with the corresponding fraction.\n",
    "\n",
    "outdf = lyft_rides.groupby(['weather', 'hour'], as_index=False)['gasoline_cost'].count().rename(columns={'gasoline_cost': 'count'})\n",
    "outdf['probability'] = outdf['count']/ len(lyft_rides)\n",
    "outdf = outdf[['weather', 'hour', 'probability']].sort_values(['weather', 'hour'])\n",
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369639b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output ids of students with a median score from the writing SAT.\n",
    "df = sat_scores[sat_scores['sat_writing'] == sat_scores['sat_writing'].median()]['student_id']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many different origin airports exist?\n",
    "\n",
    "us_flights['origin'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all emails with duplicates.\n",
    "\n",
    "employee = employee[['email', 'id']]\n",
    "employee = employee.groupby('email', as_index=False)['id'].count().rename(columns={'id':'count'})\n",
    "employee[employee['count']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f19588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average number of days between the booking and check-in dates for AirBnB hosts. \n",
    "# Order the results based on the average number of days in descending order.\n",
    "# avg_days_between_booking_and_checkin DESC\n",
    "\n",
    "df = airbnb_contacts\n",
    "df['days_between_booking_and_checkin'] = (df['ds_checkin'].dt.date - df['ts_booking_at'].dt.date).dt.days\n",
    "df = df.groupby('id_host')['days_between_booking_and_checkin'].mean().to_frame('avg_days_between_booking_and_checkin').reset_index()\n",
    "df = df[['id_host', 'avg_days_between_booking_and_checkin']].sort_values('avg_days_between_booking_and_checkin', ascending=False).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c903b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the users who were active for 3 consecutive days or more.\n",
    "\n",
    "\n",
    "# Assuming df contains the DataFrame sf_events\n",
    "df = sf_events\n",
    "# Create an empty dictionary to store user activity\n",
    "user_dates = {}\n",
    "\n",
    "# Populate the dictionary with user activity\n",
    "for i in range(len(df)):\n",
    "    user_id = df.loc[i, 'user_id']\n",
    "    date = df.loc[i, 'date']\n",
    "    \n",
    "    if user_id not in user_dates:\n",
    "        user_dates[user_id] = []\n",
    "    \n",
    "    user_dates[user_id].append(date)\n",
    "\n",
    "# Filter users who were active for 3 consecutive days or more\n",
    "active_users = {}\n",
    "for user_id, dates in user_dates.items():\n",
    "    consecutive_days_count = 1\n",
    "    for i in range(len(dates) - 1):\n",
    "        if (dates[i + 1] - dates[i]).days == 1:\n",
    "            consecutive_days_count += 1\n",
    "            if consecutive_days_count >= 3:\n",
    "                active_users[user_id] = dates[:3]  # Store the first 3 consecutive days\n",
    "                break\n",
    "        else:\n",
    "            consecutive_days_count = 1\n",
    "\n",
    "# Display the active users and their first 3 consecutive days\n",
    "print(pd.DataFrame(active_users.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 5 highest paid and top 5 least paid employees in 2012.\n",
    "# Output the employee name along with the corresponding total pay with benefits.\n",
    "# Sort records based on the total payment with benefits in ascending order.\n",
    "\n",
    "df = sf_public_salaries\n",
    "df = df[df['year'] == 2012]\n",
    "df = df[['id', 'totalpaybenefits']]\n",
    "#df = pd.merge(df.sort_values('totalpaybenefits', ascending=False)[:5], df.sort_values('totalpaybenefits')[:5])\n",
    "df_highest = df.sort_values('totalpaybenefits', ascending=False)[:5]\n",
    "df_lowest = df.sort_values('totalpaybenefits', ascending=False)[-5:]\n",
    "df_output = pd.concat([df_highest, df_lowest]).sort_values('totalpaybenefits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank each host based on the number of beds they have listed. \n",
    "# The host with the most beds should be ranked 1 and the host with the least number of beds should be ranked last. \n",
    "# Hosts that have the same number of beds should have the same rank but there should be no gaps between ranking values. \n",
    "# A host can also own multiple properties.\n",
    "# Output the host ID, number of beds, and rank from highest rank to lowest.\n",
    "\n",
    "df = airbnb_apartments\n",
    "df = df[['host_id', 'n_beds']]\n",
    "df= df.groupby('host_id',as_index=False)['n_beds'].sum()\n",
    "df = df.sort_values('n_beds', ascending=False)\n",
    "df['rank'] = df['n_beds'].rank(method='dense', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49094bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique transactions and total sales for each of the product categories in 2017. \n",
    "# Output the product categories, number of transactions, and total sales in descending order. \n",
    "# The sales column represents the total cost the customer paid for the product so no additional calculations need to be done on the column.\n",
    "# Only include product categories that have products sold.\n",
    "\n",
    "df = pd.merge(wfm_transactions, wfm_products, on='product_id', how='inner')\n",
    "df = df[df['transaction_date'].dt.year == 2017]\n",
    "df = df.groupby('product_category', as_index=False).agg({'transaction_id': ['nunique'], 'sales': ['sum']})\n",
    "df.columns = df.columns.droplevel(1)\n",
    "df = df.sort_values('sales', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query that returns a table containing the number of signups for each weekday and for each billing cycle frequency. \n",
    "# The day of the week standard we expect is from Sunday as 0 to Saturday as 6.\n",
    "# Output the weekday number (e.g., 1, 2, 3) as rows in your table and the billing cycle frequency (e.g., annual, monthly, quarterly) as columns. \n",
    "# If there are NULLs in the output replace them with zeroes.\n",
    "\n",
    "df = pd.merge(signups, plans, left_on='plan_id', right_on='id', how='inner')\n",
    "df['weekday'] = df['signup_start_date'].dt.weekday\n",
    "df = df.groupby(['weekday', 'billing_cycle'], as_index=False)['signup_id'].count()\n",
    "df = pd.pivot_table(data = df, columns='billing_cycle', index='weekday', values='signup_id').reset_index().fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d21ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most profitable location. Write a query that calculates the average signup duration \n",
    "# and average transaction amount for each location, and then compare these two measures together \n",
    "# by taking the ratio of the average transaction amount and average duration for each location.\n",
    "# Your output should include the location, average duration, average transaction amount, and ratio. \n",
    "# Sort your results from highest ratio to lowest.\n",
    "\n",
    "df = pd.merge(signups, transactions, on='signup_id', how='inner')\n",
    "df['signup_duration'] = (df['signup_stop_date'] - df['signup_start_date']).dt.days\n",
    "df = df.groupby('location', as_index=False).agg({'amt': 'mean', 'signup_duration': 'mean'})\n",
    "df['ratio'] = df['amt'] / df['signup_duration']\n",
    "df = df.sort_values('ratio', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb16708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the monthly retention rate of users for each account separately for Dec 2020 and Jan 2021. \n",
    "# Retention rate is the percentage of active users an account retains over a given period of time. \n",
    "# In this case, assume the user is retained if he/she stays with the app in any future months. \n",
    "# For example, if a user was active in Dec 2020 and has activity in any future month, consider them retained for Dec. \n",
    "# You can assume all accounts are present in Dec 2020 and Jan 2021. \n",
    "# Your output should have the account ID and the Jan 2021 retention rate divided by Dec 2020 retention rate.\n",
    "\n",
    "# Get Accounts active in December\n",
    "dec2020 = sf_events[(sf_events['date'].dt.year == 2020) & (sf_events\n",
    "['date'].dt.month == 12)].drop_duplicates(subset = ['account_id', 'user_id'])\n",
    "dec2020['in_dec'] = 1\n",
    " \n",
    "# Get accounts active after December\n",
    "aft_dec2020 = sf_events[sf_events['date'].dt.date > pd.to_datetime\n",
    "('2020-12-31')].drop_duplicates(subset = ['account_id', 'user_id'])\n",
    "aft_dec2020['aft_dec'] = 1\n",
    " \n",
    "# Merge the datasets to get December retention numbers\n",
    "dec_merged = pd.merge(dec2020, aft_dec2020, on = \n",
    "['account_id', 'user_id'], how = 'left')\n",
    "dec_summ = dec_merged.groupby(by = ['account_id'], as_index = False).sum()\n",
    "dec_summ['dec_retention'] = dec_summ['aft_dec'] / dec_summ['in_dec']\n",
    "\n",
    "# Repeat the process for Jan 2021\n",
    "jan2021 = sf_events[(sf_events['date'].dt.year == 2021) & (sf_events\n",
    "['date'].dt.month == 1)].drop_duplicates(subset = ['account_id', 'user_id'])\n",
    "jan2021['in_jan'] = 1\n",
    "aft_jan2021 = sf_events[sf_events['date'].dt.date > pd.to_datetime\n",
    "('2021-01-31')].drop_duplicates(subset = ['account_id', 'user_id'])\n",
    "aft_jan2021['aft_jan'] = 1\n",
    " \n",
    "jan_merged = pd.merge(jan2021, aft_jan2021, on = \n",
    "['account_id', 'user_id'], how = 'left')\n",
    "jan_summ = jan_merged.groupby(by = ['account_id'], as_index = False).sum()\n",
    "jan_summ['jan_retention'] = jan_summ['aft_jan'] / jan_summ['in_jan']\n",
    " \n",
    "# Merge the Dec20 and Jan21 datasets, calculate the relative retention rate \n",
    "# and output.\n",
    "final_merge = pd.merge(dec_summ, jan_summ, on = 'account_id', how = 'left')\n",
    "final_merge['retention'] = final_merge['jan_retention'] / final_merge['dec_retention']\n",
    "final_merge[['account_id', 'retention']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC Corp is a mid-sized insurer in the US and in the recent past their fraudulent claims have increased significantly for their personal auto insurance portfolio. \n",
    "# They have developed a ML based predictive model to identify propensity of fraudulent claims. \n",
    "# Now, they assign highly experienced claim adjusters for top 5 percentile of claims identified by the model.\n",
    "# Your objective is to identify the top 5 percentile of claims from each state. \n",
    "# Your output should be policy number, state, claim cost, and fraud score.\n",
    "\n",
    "fraud_score['percentile'] = fraud_score.groupby('state', as_index=False)['fraud_score'].rank(pct=True)\n",
    "fraud_score = fraud_score[fraud_score['percentile']>0.95]\n",
    "fraud_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d53dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each service, calculate the percentage of incomplete orders along with the revenue loss percentage. \n",
    "# Your output should include the name of the service, percentage of incomplete orders, and revenue loss from the incomplete orders.\n",
    "\n",
    "completed = uber_orders[uber_orders['status_of_order'] == 'Completed']\n",
    "incomplete = uber_orders[uber_orders['status_of_order'] != 'Completed']\n",
    "complete = completed.groupby('service_name', as_index=False).agg({'number_of_orders': 'sum', 'monetary_value': 'sum'})\n",
    "incomplete = incomplete.groupby('service_name', as_index=False).agg({'number_of_orders': 'sum', 'monetary_value': 'sum'})\n",
    "merged = pd.merge(complete, incomplete, on='service_name')\n",
    "merged['percent_of_incomplete'] = (merged['number_of_orders_y'] / merged['number_of_orders_x']) * 100\n",
    "merged['rev_loss'] = (merged['monetary_value_y'] / (merged['monetary_value_y'] + merged['monetary_value_x'])) * 100\n",
    "\n",
    "merged = merged.drop(columns=['number_of_orders_x', 'monetary_value_x', 'number_of_orders_y',\t'monetary_value_y'])\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some forecasting methods are extremely simple and surprisingly effective. \n",
    "# Naïve forecast is one of them; we simply set all forecasts to be the value of the last observation. \n",
    "# Our goal is to develop a naïve forecast for a new metric called \"distance per dollar\" defined as the (distance_to_travel/monetary_cost) in our dataset and measure its accuracy.\n",
    "# To develop this forecast,  sum \"distance to travel\"  and \"monetary cost\" values at a monthly level before calculating \"distance per dollar\". \n",
    "# This value becomes your actual value for the current month. \n",
    "# The next step is to populate the forecasted value for each month. \n",
    "# This can be achieved simply by getting the previous month's value in a separate column. \n",
    "# Now, we have actual and forecasted values. This is your naïve forecast. \n",
    "# Let’s evaluate our model by calculating an error matrix called root mean squared error (RMSE). \n",
    "# RMSE is defined as sqrt(mean(square(actual - forecast)). \n",
    "# Report out the RMSE rounded to the 2nd decimal spot.\n",
    "\n",
    "df = uber_request_logs\n",
    "df = df[['request_date', 'distance_to_travel', 'monetary_cost']]\n",
    "df['month'] = df['request_date'].dt.month\n",
    "df = df.groupby('month', as_index=False).agg({'distance_to_travel': 'sum', 'monetary_cost':'sum'})\n",
    "df['distance_per_dollar'] = df['distance_to_travel'] / df['monetary_cost']\n",
    "df['forecast'] = df['distance_per_dollar'].shift(1).fillna(0)\n",
    "\n",
    "rmse = round((((df['distance_per_dollar'] - df['forecast'])**2).mean())**0.5, 2)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top business categories based on the total number of reviews. \n",
    "# Output the category along with the total number of reviews. \n",
    "# Order by total reviews in descending order.\n",
    "\n",
    "df = yelp_business\n",
    "df['categories'] = df['categories'].apply(lambda x: x.split(';'))\n",
    "df = df[['categories', 'review_count']]\n",
    "df = df.explode('categories').groupby('categories', as_index=False).sum().sort_values('review_count', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75175359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of times each word appears in drafts.\n",
    "# Output the word along with the corresponding number of occurrences.\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return ''.join(char for char in x if char not in string.punctuation)\n",
    "    \n",
    "df = google_file_store\n",
    "df = df[df['filename'].str.contains('draft')]\n",
    "df['contents'] = df['contents'].apply(remove_punctuation).apply(lambda x: x.split(\" \"))\n",
    "df = df.explode('contents')\n",
    "df['count'] = [1]*len(df)\n",
    "df = df.groupby('contents', as_index=False)['count'].sum()\n",
    "df\n",
    "\n",
    "#OR\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "draft = google_file_store[google_file_store['filename'].str.contains('draft')]\n",
    "result = draft.contents.str.split('\\W+', expand=True).stack().value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cf898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The company you work for has asked you to look into the average order value per hour \n",
    "# during rush hours in the San Jose area. \n",
    "# Rush hour is from 15H - 17H inclusive.\n",
    "# You have also been told that the column order_total represents the gross order total for each order. \n",
    "# Therefore, you'll need to calculate the net order total.\n",
    "# The gross order total is the total of the order before adding the tip and deducting the discount and refund.\n",
    "# Use the column customer_placed_order_datetime for your calculations.\n",
    "\n",
    "df = delivery_details[delivery_details['delivery_region'] == 'San Jose']\n",
    "df = df[['customer_placed_order_datetime', 'order_total', 'discount_amount', 'tip_amount',\t 'refunded_amount']]\n",
    "df['net_total'] = df['order_total'] + df['tip_amount'] - df['discount_amount'] - df['refunded_amount']\n",
    "df = df.drop(['order_total', 'discount_amount', 'tip_amount',\t 'refunded_amount'], axis=1)\n",
    "df['peak_hours'] = df['customer_placed_order_datetime'].dt.hour\n",
    "df = df[(df['peak_hours'] >=15) &  (df['peak_hours'] <=17)]\n",
    "df = df.groupby('peak_hours')['net_total'].mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. \n",
    "# The user with the highest number of emails sent will have a rank of 1, and so on. \n",
    "# Output the user, total emails, and their activity rank. \n",
    "# Order records by the total emails in descending order. \n",
    "# Sort users with the same number of emails in alphabetical order.\n",
    "# In your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails. \n",
    "# For tie breaker use alphabetical order of the user usernames.\n",
    "\n",
    "df = google_gmail_emails\n",
    "df['count'] = [1]*len(df)\n",
    "df = df.groupby('from_user', as_index=False)['count'].sum().sort_values(['count', 'from_user'], ascending= [False, True])\n",
    "df['rank'] = [i for i in range(1, len(df)+1)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c92ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query that returns a number of users who are exclusive to only one client. \n",
    "# Output the client_id and number of exclusive users.\n",
    "\n",
    "df = fact_events[['user_id', 'client_id']]\n",
    "num_users = df.groupby('user_id', as_index=False)['client_id'].nunique()\n",
    "exclusive = num_users[num_users['client_id']==1]\n",
    "exclusive = exclusive.drop(columns = 'client_id')\n",
    "exclusive = pd.merge(exclusive, df, on='user_id')\n",
    "exclusive = exclusive.groupby('client_id', as_index=False)['user_id'].nunique()\n",
    "exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf852f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the win-to-nomination ratio for each nominee. Output the ratio and the nominee's name. \n",
    "# Order the results based on the ratio in descending order to show nominees with the highest ratio on top.\n",
    "\n",
    "df = oscar_nominees[['nominee', 'winner']]\n",
    "df = pd.pivot_table(columns='winner', index='nominee', data=df, aggfunc=len, fill_value=0).reset_index()\n",
    "df['ratio'] = df[True] / (df[True] + df[False])\n",
    "df = df.sort_values('ratio', ascending=False).drop(columns=[False, True])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query that returns an array of signup ids by day. Order by most signups in descending order.\n",
    "\n",
    "df = signups[['signup_id', 'signup_start_date']]\n",
    "df['signup_start_date'] = df['signup_start_date'].dt.date\n",
    "df = df.groupby(by = ['signup_start_date'])['signup_id'].apply(list).reset_index()\n",
    "df['num_signups'] = df['signup_id'].apply(len)\n",
    "df = df.sort_values(['num_signups', 'signup_start_date'], ascending=[False, True]).drop(columns='num_signups')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ccfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You work for a multinational company that wants to calculate total sales across all their countries they do business in.\n",
    "# You have 2 tables, one is a record of sales for all countries and currencies the company deals with, and the other holds currency exchange rate information.\n",
    "# Calculate the total sales, per quarter, for the first 2 quarters in 2020, and report the sales in USD currency.\n",
    "\n",
    "exng = sf_exchange_rate\n",
    "sales = sf_sales_amount\n",
    "sales = sales[sales['sales_date'].dt.year == 2020]\n",
    "sales['quarter'] = sales['sales_date'].dt.quarter\n",
    "sales = sales[(sales['quarter'] == 1) | (sales['quarter'] == 2)]\n",
    "sales['sales_date'] = sales['sales_date'].dt.date\n",
    "exng['date'] = exng['date'].dt.date\n",
    "sales = pd.merge(sales, exng, left_on=['sales_date', 'currency'], right_on=['date', 'source_currency'], how=\"left\").reset_index()\n",
    "sales['amount'] = sales['sales_amount'] * sales['exchange_rate']\n",
    "sales = sales[['quarter', 'amount']]\n",
    "sales = sales.groupby('quarter', as_index=False)['amount'].sum()\n",
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From users who had their first session as a viewer, how many streamer sessions have they had? \n",
    "# Return the user id and number of sessions in descending order. \n",
    "# In case there are users with the same number of sessions, order them by ascending user id.\n",
    "\n",
    "df = twitch_sessions\n",
    "df = df.sort_values(['user_id', 'session_start'], ascending=[True, True])\n",
    "df1 = df.groupby('user_id')['session_type'].apply(list).reset_index()\n",
    "users = []\n",
    "for i in range(len(df1)):\n",
    "    if df1.iloc[i,1][0] == 'viewer':\n",
    "        users.append(df1.iloc[i,0])\n",
    "        \n",
    "df = df[df['user_id'].isin(users)]\n",
    "df = df.groupby('user_id', as_index=False)['session_id'].count()\n",
    "df['session_id'] = df['session_id'] - 1\n",
    "df = df.sort_values('user_id', ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You’re given a table of Uber rides that contains the mileage and the purpose for the business expense.  \n",
    "# You’re asked to find business purposes that generate the most miles driven for passengers that use Uber for their business transportation. \n",
    "# Find the top 3 business purpose categories by total mileage.\n",
    "\n",
    "df = my_uber_drives\n",
    "df = df[df['category'] =='Business']\n",
    "df = df.dropna(subset=['purpose'])\n",
    "df = df.groupby('purpose', as_index=False)['miles'].sum()\n",
    "df = df.sort_values('miles', ascending=False)[:3]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
